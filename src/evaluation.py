import os
import torch
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt
import logging
import numpy as np


def evaluate_model(model, dataloader, device, classes, save_dir="./plots_results"):
    """
    Evaluate the trained model on a test dataset.
    """
    logging.info("Starting evaluation...")
    model.to(device)
    model.eval()

    all_preds = []
    all_labels = []

    # Collect predictions
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Verify unique classes in predictions and labels
    unique_labels = np.unique(all_labels)
    unique_preds = np.unique(all_preds)
    logging.info(f"Unique labels in test set: {unique_labels}")
    logging.info(f"Unique predictions: {unique_preds}")

    # Filter classes to only include those present in the test set
    present_classes = sorted(list(set([classes[i] for i in unique_labels])))
    logging.info(f"Classes present in test set: {present_classes}")

    # Calculate metrics
    try:
        report = classification_report(
            all_labels,
            all_preds,
            labels=range(len(present_classes)),  # Use indices
            target_names=present_classes,        # Use present classes
            output_dict=True,
            zero_division=0
        )

        print("Classification Report:")
        print(classification_report(
            all_labels, all_preds,
            labels=range(len(present_classes)),
            target_names=present_classes,
            zero_division=0
        ))

        # Generate confusion matrix
        cm = confusion_matrix(all_labels, all_preds, labels=range(len(present_classes)))

        # Save plots
        os.makedirs(save_dir, exist_ok=True)
        save_plot(plot_confusion_matrix, save_dir, "confusion_matrix.png",
                 cm=cm, class_names=present_classes)

        # Calculate ROC curves only for present classes
        roc_auc = plot_roc_curves(all_labels, all_preds, present_classes, save_dir)

        return {
            "accuracy": report["accuracy"],
            "macro_precision": report["macro avg"]["precision"],
            "macro_recall": report["macro avg"]["recall"],
            "macro_f1": report["macro avg"]["f1-score"],
            "roc_auc": roc_auc,
        }

    except Exception as e:
        logging.error(f"Error in evaluation metrics calculation: {e}")
        raise


def save_plot(plot_fn, save_dir, filename, **kwargs):
    """
    Save a plot generated by a plot function.
    """
    # Ensure the save directory exists
    os.makedirs(save_dir, exist_ok=True)

    save_path = os.path.join(save_dir, filename)
    plot_fn(**kwargs)  # Call the plot function
    plt.savefig(save_path)
    logging.info(f"Saved plot to {save_path}")
    plt.close()  # Close the plot to free memory

def plot_confusion_matrix(cm, class_names):
    """
    Plot the confusion matrix.
    """
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names, cmap="Blues")
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")


def plot_roc_curves(all_labels, all_preds, class_names, save_dir):
    """
    Plot ROC curves and calculate AUC for each class.
    """
    fpr, tpr, roc_auc = {}, {}, {}
    for i, cls in enumerate(class_names):
        labels_bin = [1 if label == i else 0 for label in all_labels]
        preds_bin = [1 if pred == i else 0 for pred in all_preds]
        if sum(labels_bin) > 0:  # Avoid cases where a class is missing
            fpr[i], tpr[i], _ = roc_curve(labels_bin, preds_bin)
            roc_auc[i] = auc(fpr[i], tpr[i])
        else:
            fpr[i], tpr[i], roc_auc[i] = [0], [0], 0.0

    plt.figure(figsize=(10, 8))
    for i, cls in enumerate(class_names):
        if roc_auc[i] > 0:  # Plot only valid ROC curves
            plt.plot(fpr[i], tpr[i], label=f"ROC curve for {cls} (AUC = {roc_auc[i]:.2f})")
    plt.plot([0, 1], [0, 1], "k--")  # Random guess line
    plt.title("Receiver Operating Characteristic")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")

    save_path = os.path.join(save_dir, "roc_curves.png")
    plt.savefig(save_path)
    logging.info(f"Saved plot to {save_path}")
    plt.close()

    return roc_auc